{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import remove_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"D:\\Data science\\Projects docs\\Project chatbot\\Chatbot_data.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Questions</th>\n",
       "      <th>Answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1. What Is A Recommender System</td>\n",
       "      <td>A recommender system is today widely deployed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2. Compare Sas, R and Python Programming</td>\n",
       "      <td>SAS: it is one of the most widely used analyt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3. Explain the Various Benefits of R Language</td>\n",
       "      <td>The R programming language includes a set of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4. How Do Data Scientists Use Statistics</td>\n",
       "      <td>Statistics helps Data Scientists to look into...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5. What Is Logistic Regression</td>\n",
       "      <td>It is a statistical technique or a model in o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Questions  \\\n",
       "0                1. What Is A Recommender System   \n",
       "1       2. Compare Sas, R and Python Programming   \n",
       "2  3. Explain the Various Benefits of R Language   \n",
       "3       4. How Do Data Scientists Use Statistics   \n",
       "4                 5. What Is Logistic Regression   \n",
       "\n",
       "                                             Answers  \n",
       "0   A recommender system is today widely deployed...  \n",
       "1   SAS: it is one of the most widely used analyt...  \n",
       "2   The R programming language includes a set of ...  \n",
       "3   Statistics helps Data Scientists to look into...  \n",
       "4   It is a statistical technique or a model in o...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(92, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 92 entries, 0 to 91\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   Questions  92 non-null     object\n",
      " 1   Answers    92 non-null     object\n",
      "dtypes: object(2)\n",
      "memory usage: 1.6+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentences(sentence, stop_words = False):\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = re.sub('[^a-z0-9\\s]', '', sentence)\n",
    "    \n",
    "    if stop_words:\n",
    "        sentence = remove_stopwords(sentence)\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cleaned_sentences(data, stop_words = False):\n",
    "    sents = data[[\"Questions\"]]\n",
    "    cleaned_sentences = []\n",
    "    \n",
    "    for index, row in data.iterrows():\n",
    "        cleaned = clean_sentences(row[\"Questions\"], stop_words)\n",
    "        cleaned = re.sub(\"[0-9]\", \"\", cleaned)\n",
    "        cleaned_sentences.append(cleaned)\n",
    "    return cleaned_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned questions without stop words\n",
    "cleaned_sentences = get_cleaned_sentences(data, stop_words = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' recommender',\n",
       " ' compare sas r python programming',\n",
       " ' explain benefits r language',\n",
       " ' data scientists use statistics',\n",
       " ' logistic regression',\n",
       " ' data cleansing important data analysis',\n",
       " ' univariate bivariate multivariate analysis',\n",
       " ' machine learning deployed real world scenarios',\n",
       " ' aspects machine learning process',\n",
       " ' understand term normal distribution']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_sentences[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned questions with stopwords\n",
    "cleaned_sentenes_with_stopwords = get_cleaned_sentences(data, stop_words = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' what is a recommender system',\n",
       " ' compare sas r and python programming',\n",
       " ' explain the various benefits of r language',\n",
       " ' how do data scientists use statistics',\n",
       " ' what is logistic regression',\n",
       " ' why data cleansing is important in data analysis',\n",
       " ' describe univariate bivariate and multivariate analysis',\n",
       " ' how machine learning is deployed in real world scenarios',\n",
       " ' what are the various aspects of a machine learning process',\n",
       " ' what do you understand by the term normal distribution']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_sentenes_with_stopwords[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = cleaned_sentenes_with_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the words by white space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_words = [[word for word in document.split()] for document in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dictionary of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(sentence_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : a\n",
      "1 : is\n",
      "2 : recommender\n",
      "3 : system\n",
      "4 : what\n",
      "5 : and\n",
      "6 : compare\n",
      "7 : programming\n",
      "8 : python\n",
      "9 : r\n",
      "10 : sas\n",
      "11 : benefits\n",
      "12 : explain\n",
      "13 : language\n",
      "14 : of\n",
      "15 : the\n",
      "16 : various\n",
      "17 : data\n",
      "18 : do\n",
      "19 : how\n",
      "20 : scientists\n",
      "21 : statistics\n",
      "22 : use\n",
      "23 : logistic\n",
      "24 : regression\n",
      "25 : analysis\n",
      "26 : cleansing\n",
      "27 : important\n",
      "28 : in\n",
      "29 : why\n",
      "30 : bivariate\n",
      "31 : describe\n",
      "32 : multivariate\n",
      "33 : univariate\n",
      "34 : deployed\n",
      "35 : learning\n",
      "36 : machine\n",
      "37 : real\n",
      "38 : scenarios\n",
      "39 : world\n",
      "40 : are\n",
      "41 : aspects\n",
      "42 : process\n",
      "43 : by\n",
      "44 : distribution\n",
      "45 : normal\n",
      "46 : term\n",
      "47 : understand\n",
      "48 : you\n",
      "49 : linear\n",
      "50 : extrapolation\n",
      "51 : interpolation\n",
      "52 : power\n",
      "53 : can\n",
      "54 : for\n",
      "55 : k\n",
      "56 : kmeans\n",
      "57 : select\n",
      "58 : database\n",
      "59 : design\n",
      "60 : different\n",
      "61 : from\n",
      "62 : modeling\n",
      "63 : feature\n",
      "64 : vectors\n",
      "65 : decision\n",
      "66 : making\n",
      "67 : steps\n",
      "68 : tree\n",
      "69 : cause\n",
      "70 : root\n",
      "71 : crossvalidation\n",
      "72 : collaborative\n",
      "73 : filtering\n",
      "74 : all\n",
      "75 : at\n",
      "76 : converge\n",
      "77 : descent\n",
      "78 : gradient\n",
      "79 : methods\n",
      "80 : point\n",
      "81 : similar\n",
      "82 : times\n",
      "83 : to\n",
      "84 : ab\n",
      "85 : goal\n",
      "86 : testing\n",
      "87 : drawbacks\n",
      "88 : model\n",
      "89 : large\n",
      "90 : law\n",
      "91 : numbers\n",
      "92 : confounding\n",
      "93 : variables\n",
      "94 : schema\n",
      "95 : star\n",
      "96 : algorithm\n",
      "97 : an\n",
      "98 : be\n",
      "99 : must\n",
      "100 : regularly\n",
      "101 : update\n",
      "102 : eigenvalue\n",
      "103 : eigenvector\n",
      "104 : done\n",
      "105 : resampling\n",
      "106 : bias\n",
      "107 : selective\n",
      "108 : biases\n",
      "109 : during\n",
      "110 : occur\n",
      "111 : sampling\n",
      "112 : that\n",
      "113 : types\n",
      "114 : forest\n",
      "115 : random\n",
      "116 : towards\n",
      "117 : work\n",
      "118 : analytics\n",
      "119 : one\n",
      "120 : or\n",
      "121 : prefer\n",
      "122 : text\n",
      "123 : which\n",
      "124 : would\n",
      "125 : example\n",
      "126 : have\n",
      "127 : recently\n",
      "128 : state\n",
      "129 : used\n",
      "130 : when\n",
      "131 : systems\n",
      "132 : cleaning\n",
      "133 : plays\n",
      "134 : role\n",
      "135 : vital\n",
      "136 : between\n",
      "137 : differentiate\n",
      "138 : cluster\n",
      "139 : difference\n",
      "140 : systematic\n",
      "141 : expected\n",
      "142 : mean\n",
      "143 : value\n",
      "144 : about\n",
      "145 : does\n",
      "146 : pvalue\n",
      "147 : signify\n",
      "148 : statistical\n",
      "149 : always\n",
      "150 : same\n",
      "151 : categorical\n",
      "152 : boxcox\n",
      "153 : make\n",
      "154 : transformation\n",
      "155 : using\n",
      "156 : supervised\n",
      "157 : unsupervised\n",
      "158 : combinatorics\n",
      "159 : science\n",
      "160 : vectorization\n",
      "161 : outlier\n",
      "162 : treated\n",
      "163 : values\n",
      "164 : assess\n",
      "165 : good\n",
      "166 : involved\n",
      "167 : project\n",
      "168 : also\n",
      "169 : element\n",
      "170 : indices\n",
      "171 : iterate\n",
      "172 : list\n",
      "173 : over\n",
      "174 : retrieve\n",
      "175 : time\n",
      "176 : missing\n",
      "177 : treat\n",
      "178 : box\n",
      "179 : cox\n",
      "180 : models\n",
      "181 : series\n",
      "182 : bayesian\n",
      "183 : estimate\n",
      "184 : estimation\n",
      "185 : likelihood\n",
      "186 : maximum\n",
      "187 : mle\n",
      "188 : kind\n",
      "189 : problems\n",
      "190 : regularization\n",
      "191 : solve\n",
      "192 : it\n",
      "193 : multicollinearity\n",
      "194 : overcome\n",
      "195 : curse\n",
      "196 : dimensionality\n",
      "197 : decide\n",
      "198 : fits\n",
      "199 : whether\n",
      "200 : your\n",
      "201 : absolute\n",
      "202 : error\n",
      "203 : squared\n",
      "204 : confidence\n",
      "205 : constructed\n",
      "206 : interpret\n",
      "207 : intervals\n",
      "208 : them\n",
      "209 : will\n",
      "210 : overfitting\n",
      "211 : formats\n",
      "212 : tall\n",
      "213 : wide\n",
      "214 : clustering\n",
      "215 : clusters\n",
      "216 : define\n",
      "217 : number\n",
      "218 : better\n",
      "219 : false\n",
      "220 : many\n",
      "221 : negatives\n",
      "222 : positives\n",
      "223 : too\n",
      "224 : fuzzy\n",
      "225 : handle\n",
      "226 : merging\n",
      "227 : skewed\n",
      "228 : uniform\n",
      "229 : created\n",
      "230 : follow\n",
      "231 : multiple\n",
      "232 : outcome\n",
      "233 : predictive\n",
      "234 : quantitative\n",
      "235 : regressions\n",
      "236 : validate\n",
      "237 : variable\n",
      "238 : content\n",
      "239 : hypothesis\n",
      "240 : precision\n",
      "241 : recall\n",
      "242 : find\n",
      "243 : right\n",
      "244 : causes\n",
      "245 : l\n",
      "246 : not\n",
      "247 : parameter\n",
      "248 : regularizations\n",
      "249 : sparsity\n",
      "250 : whereas\n",
      "251 : deal\n",
      "252 : modelling\n",
      "253 : seasonality\n",
      "254 : with\n",
      "255 : experimental\n",
      "256 : if\n",
      "257 : necessary\n",
      "258 : randomization\n",
      "259 : yes\n",
      "260 : cite\n",
      "261 : examples\n",
      "262 : negative\n",
      "263 : positive\n",
      "264 : some\n",
      "265 : than\n",
      "266 : where\n",
      "267 : both\n",
      "268 : equally\n",
      "269 : set\n",
      "270 : test\n",
      "271 : validation\n",
      "272 : dataset\n",
      "273 : gold\n",
      "274 : makes\n",
      "275 : standard\n",
      "276 : calculate\n",
      "277 : sensitivity\n",
      "278 : having\n",
      "279 : importance\n",
      "280 : selection\n",
      "281 : give\n",
      "282 : situations\n",
      "283 : svm\n",
      "284 : viceversa\n",
      "285 : handling\n",
      "286 : like\n",
      "287 : management\n",
      "288 : procedures\n",
      "289 : worse\n"
     ]
    }
   ],
   "source": [
    "for key, value in dictionary.items():\n",
    "    print(key, \":\", value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(text) for text in sentence_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " what is a recommender system\n",
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)]\n",
      "\n",
      " compare sas r and python programming\n",
      "[(5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1)]\n",
      "\n",
      " explain the various benefits of r language\n",
      "[(9, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1)]\n",
      "\n",
      " how do data scientists use statistics\n",
      "[(17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1)]\n",
      "\n",
      " what is logistic regression\n",
      "[(1, 1), (4, 1), (23, 1), (24, 1)]\n",
      "\n",
      " why data cleansing is important in data analysis\n",
      "[(1, 1), (17, 2), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1)]\n",
      "\n",
      " describe univariate bivariate and multivariate analysis\n",
      "[(5, 1), (25, 1), (30, 1), (31, 1), (32, 1), (33, 1)]\n",
      "\n",
      " how machine learning is deployed in real world scenarios\n",
      "[(1, 1), (19, 1), (28, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1)]\n",
      "\n",
      " what are the various aspects of a machine learning process\n",
      "[(0, 1), (4, 1), (14, 1), (15, 1), (16, 1), (35, 1), (36, 1), (40, 1), (41, 1), (42, 1)]\n",
      "\n",
      " what do you understand by the term normal distribution\n",
      "[(4, 1), (15, 1), (18, 1), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 1)]\n",
      "\n",
      " what is linear regression\n",
      "[(1, 1), (4, 1), (24, 1), (49, 1)]\n",
      "\n",
      " what is interpolation and extrapolation\n",
      "[(1, 1), (4, 1), (5, 1), (50, 1), (51, 1)]\n",
      "\n",
      " what is power analysis\n",
      "[(1, 1), (4, 1), (25, 1), (52, 1)]\n",
      "\n",
      " what is kmeans  how can you select k for kmeans\n",
      "[(1, 1), (4, 1), (19, 1), (48, 1), (53, 1), (54, 1), (55, 1), (56, 2), (57, 1)]\n",
      "\n",
      " how is data modeling different from database design\n",
      "[(1, 1), (17, 1), (19, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1)]\n",
      "\n",
      " what are feature vectors\n",
      "[(4, 1), (40, 1), (63, 1), (64, 1)]\n",
      "\n",
      " explain the steps in making a decision tree\n",
      "[(0, 1), (12, 1), (15, 1), (28, 1), (65, 1), (66, 1), (67, 1), (68, 1)]\n",
      "\n",
      " what is root cause analysis\n",
      "[(1, 1), (4, 1), (25, 1), (69, 1), (70, 1)]\n",
      "\n",
      " explain crossvalidation\n",
      "[(12, 1), (71, 1)]\n",
      "\n",
      " what is collaborative filtering\n",
      "[(1, 1), (4, 1), (72, 1), (73, 1)]\n",
      "\n",
      " do gradient descent methods at all times converge to similar point\n",
      "[(18, 1), (74, 1), (75, 1), (76, 1), (77, 1), (78, 1), (79, 1), (80, 1), (81, 1), (82, 1), (83, 1)]\n",
      "\n",
      " what is the goal of ab testing\n",
      "[(1, 1), (4, 1), (14, 1), (15, 1), (84, 1), (85, 1), (86, 1)]\n",
      "\n",
      " what are the drawbacks of linear model\n",
      "[(4, 1), (14, 1), (15, 1), (40, 1), (49, 1), (87, 1), (88, 1)]\n",
      "\n",
      " what is the law of large numbers\n",
      "[(1, 1), (4, 1), (14, 1), (15, 1), (89, 1), (90, 1), (91, 1)]\n",
      "\n",
      " what are confounding variables\n",
      "[(4, 1), (40, 1), (92, 1), (93, 1)]\n",
      "\n",
      " explain star schema\n",
      "[(12, 1), (94, 1), (95, 1)]\n",
      "\n",
      " how regularly an algorithm must be update\n",
      "[(19, 1), (96, 1), (97, 1), (98, 1), (99, 1), (100, 1), (101, 1)]\n",
      "\n",
      " what are eigenvalue and eigenvector\n",
      "[(4, 1), (5, 1), (40, 1), (102, 1), (103, 1)]\n",
      "\n",
      " why is resampling done\n",
      "[(1, 1), (29, 1), (104, 1), (105, 1)]\n",
      "\n",
      " explain selective bias\n",
      "[(12, 1), (106, 1), (107, 1)]\n",
      "\n",
      " what are the types of biases that can occur during sampling\n",
      "[(4, 1), (14, 1), (15, 1), (40, 1), (53, 1), (108, 1), (109, 1), (110, 1), (111, 1), (112, 1), (113, 1)]\n",
      "\n",
      " how to work towards a random forest\n",
      "[(0, 1), (19, 1), (83, 1), (114, 1), (115, 1), (116, 1), (117, 1)]\n",
      "\n",
      " python or r  which one would you prefer for text analytics\n",
      "[(8, 1), (9, 1), (48, 1), (54, 1), (118, 1), (119, 1), (120, 1), (121, 1), (122, 1), (123, 1), (124, 1)]\n",
      "\n",
      " what is logistic regression or state an example when you have used logistic regression recently\n",
      "[(1, 1), (4, 1), (23, 2), (24, 2), (48, 1), (97, 1), (120, 1), (125, 1), (126, 1), (127, 1), (128, 1), (129, 1), (130, 1)]\n",
      "\n",
      " what are recommender systems\n",
      "[(2, 1), (4, 1), (40, 1), (131, 1)]\n",
      "\n",
      " why data cleaning plays a vital role in analysis\n",
      "[(0, 1), (17, 1), (25, 1), (28, 1), (29, 1), (132, 1), (133, 1), (134, 1), (135, 1)]\n",
      "\n",
      " differentiate between univariate bivariate and multivariate analysis\n",
      "[(5, 1), (25, 1), (30, 1), (32, 1), (33, 1), (136, 1), (137, 1)]\n",
      "\n",
      " what do you understand by the term normal distribution\n",
      "[(4, 1), (15, 1), (18, 1), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 1)]\n",
      "\n",
      " what is linear regression\n",
      "[(1, 1), (4, 1), (24, 1), (49, 1)]\n",
      "\n",
      " what is interpolation and extrapolation\n",
      "[(1, 1), (4, 1), (5, 1), (50, 1), (51, 1)]\n",
      "\n",
      " what is power analysis\n",
      "[(1, 1), (4, 1), (25, 1), (52, 1)]\n",
      "\n",
      " what is collaborative filtering\n",
      "[(1, 1), (4, 1), (72, 1), (73, 1)]\n",
      "\n",
      " what is the difference between cluster and systematic sampling\n",
      "[(1, 1), (4, 1), (5, 1), (15, 1), (111, 1), (136, 1), (138, 1), (139, 1), (140, 1)]\n",
      "\n",
      " are expected value and mean value different\n",
      "[(5, 1), (40, 1), (60, 1), (141, 1), (142, 1), (143, 2)]\n",
      "\n",
      " what does pvalue signify about the statistical data\n",
      "[(4, 1), (15, 1), (17, 1), (144, 1), (145, 1), (146, 1), (147, 1), (148, 1)]\n",
      "\n",
      " do gradient descent methods always converge to same point\n",
      "[(18, 1), (76, 1), (77, 1), (78, 1), (79, 1), (80, 1), (83, 1), (149, 1), (150, 1)]\n",
      "\n",
      " what are categorical variables\n",
      "[(4, 1), (40, 1), (93, 1), (151, 1)]\n",
      "\n",
      " how you can make data normal using boxcox transformation\n",
      "[(17, 1), (19, 1), (45, 1), (48, 1), (53, 1), (152, 1), (153, 1), (154, 1), (155, 1)]\n",
      "\n",
      " what is the difference between supervised learning an unsupervised learning\n",
      "[(1, 1), (4, 1), (15, 1), (35, 2), (97, 1), (136, 1), (139, 1), (156, 1), (157, 1)]\n",
      "\n",
      " explain the use of combinatorics in data science\n",
      "[(12, 1), (14, 1), (15, 1), (17, 1), (22, 1), (28, 1), (158, 1), (159, 1)]\n",
      "\n",
      " what is vectorization\n",
      "[(1, 1), (4, 1), (160, 1)]\n",
      "\n",
      " what is the goal of ab testing\n",
      "[(1, 1), (4, 1), (14, 1), (15, 1), (84, 1), (85, 1), (86, 1)]\n",
      "\n",
      " what is an eigenvalue and eigenvector\n",
      "[(1, 1), (4, 1), (5, 1), (97, 1), (102, 1), (103, 1)]\n",
      "\n",
      " what is gradient descent\n",
      "[(1, 1), (4, 1), (77, 1), (78, 1)]\n",
      "\n",
      " how can outlier values be treated\n",
      "[(19, 1), (53, 1), (98, 1), (161, 1), (162, 1), (163, 1)]\n",
      "\n",
      " how can you assess a good logistic model\n",
      "[(0, 1), (19, 1), (23, 1), (48, 1), (53, 1), (88, 1), (164, 1), (165, 1)]\n",
      "\n",
      " what are various steps involved in an analytics project\n",
      "[(4, 1), (16, 1), (28, 1), (40, 1), (67, 1), (97, 1), (118, 1), (166, 1), (167, 1)]\n",
      "\n",
      " how can you iterate over a list and also retrieve element indices at the same time\n",
      "[(0, 1), (5, 1), (15, 1), (19, 1), (48, 1), (53, 1), (75, 1), (150, 1), (168, 1), (169, 1), (170, 1), (171, 1), (172, 1), (173, 1), (174, 1), (175, 1)]\n",
      "\n",
      " during analysis how do you treat missing values\n",
      "[(18, 1), (19, 1), (25, 1), (48, 1), (109, 1), (163, 1), (176, 1), (177, 1)]\n",
      "\n",
      " explain about the box cox transformation in regression models\n",
      "[(12, 1), (15, 1), (24, 1), (28, 1), (144, 1), (154, 1), (178, 1), (179, 1), (180, 1)]\n",
      "\n",
      " can you use machine learning for time series analysis\n",
      "[(22, 1), (25, 1), (35, 1), (36, 1), (48, 1), (53, 1), (54, 1), (175, 1), (181, 1)]\n",
      "\n",
      " what is the difference between bayesian estimate and maximum likelihood estimation mle\n",
      "[(1, 1), (4, 1), (5, 1), (15, 1), (136, 1), (139, 1), (182, 1), (183, 1), (184, 1), (185, 1), (186, 1), (187, 1)]\n",
      "\n",
      " what is regularization and what kind of problems does regularization solve\n",
      "[(1, 1), (4, 2), (5, 1), (14, 1), (145, 1), (188, 1), (189, 1), (190, 2), (191, 1)]\n",
      "\n",
      " what is multicollinearity and how you can overcome it\n",
      "[(1, 1), (4, 1), (5, 1), (19, 1), (48, 1), (53, 1), (192, 1), (193, 1), (194, 1)]\n",
      "\n",
      " what is the curse of dimensionality\n",
      "[(1, 1), (4, 1), (14, 1), (15, 1), (195, 1), (196, 1)]\n",
      "\n",
      " how do you decide whether your linear regression model fits the data\n",
      "[(15, 1), (17, 1), (18, 1), (19, 1), (24, 1), (48, 1), (49, 1), (88, 1), (197, 1), (198, 1), (199, 1), (200, 1)]\n",
      "\n",
      " what is the difference between mean squared error and mean absolute error\n",
      "[(1, 1), (4, 1), (5, 1), (15, 1), (136, 1), (139, 1), (142, 2), (201, 1), (202, 2), (203, 1)]\n",
      "\n",
      " what is machine learning\n",
      "[(1, 1), (4, 1), (35, 1), (36, 1)]\n",
      "\n",
      " how are confidence intervals constructed and how will you interpret them\n",
      "[(5, 1), (19, 2), (40, 1), (48, 1), (204, 1), (205, 1), (206, 1), (207, 1), (208, 1), (209, 1)]\n",
      "\n",
      " how can you overcome overfitting\n",
      "[(19, 1), (48, 1), (53, 1), (194, 1), (210, 1)]\n",
      "\n",
      " differentiate between wide and tall data formats\n",
      "[(5, 1), (17, 1), (136, 1), (137, 1), (211, 1), (212, 1), (213, 1)]\n",
      "\n",
      " how will you define the number of clusters in a clustering algorithm\n",
      "[(0, 1), (14, 1), (15, 1), (19, 1), (28, 1), (48, 1), (96, 1), (209, 1), (214, 1), (215, 1), (216, 1), (217, 1)]\n",
      "\n",
      " is it better to have too many false negatives or too many false positives\n",
      "[(1, 1), (83, 1), (120, 1), (126, 1), (192, 1), (218, 1), (219, 2), (220, 2), (221, 1), (222, 1), (223, 2)]\n",
      "\n",
      " what do you understand by fuzzy merging which language will you use to handle it\n",
      "[(4, 1), (13, 1), (18, 1), (22, 1), (43, 1), (47, 1), (48, 2), (83, 1), (123, 1), (192, 1), (209, 1), (224, 1), (225, 1), (226, 1)]\n",
      "\n",
      " what is the difference between skewed and uniform distribution\n",
      "[(1, 1), (4, 1), (5, 1), (15, 1), (44, 1), (136, 1), (139, 1), (227, 1), (228, 1)]\n",
      "\n",
      " you created a predictive model of a quantitative outcome variable using multiple regressions what are the steps you would follow to validate the model\n",
      "[(0, 2), (4, 1), (14, 1), (15, 2), (40, 1), (48, 2), (67, 1), (83, 1), (88, 2), (124, 1), (155, 1), (229, 1), (230, 1), (231, 1), (232, 1), (233, 1), (234, 1), (235, 1), (236, 1), (237, 1)]\n",
      "\n",
      " what do you understand by hypothesis in the content of machine learning\n",
      "[(4, 1), (14, 1), (15, 1), (18, 1), (28, 1), (35, 1), (36, 1), (43, 1), (47, 1), (48, 1), (238, 1), (239, 1)]\n",
      "\n",
      " what do you understand by recall and precision\n",
      "[(4, 1), (5, 1), (18, 1), (43, 1), (47, 1), (48, 1), (240, 1), (241, 1)]\n",
      "\n",
      " how will you find the right k value for kmeans\n",
      "[(15, 1), (19, 1), (48, 1), (54, 1), (55, 1), (56, 1), (143, 1), (209, 1), (242, 1), (243, 1)]\n",
      "\n",
      " why l regularizations causes parameter sparsity whereas l regularization does not\n",
      "[(29, 1), (145, 1), (190, 1), (244, 1), (245, 2), (246, 1), (247, 1), (248, 1), (249, 1), (250, 1)]\n",
      "\n",
      " how can you deal with different types of seasonality in time series modelling\n",
      "[(14, 1), (19, 1), (28, 1), (48, 1), (53, 1), (60, 1), (113, 1), (175, 1), (181, 1), (251, 1), (252, 1), (253, 1), (254, 1)]\n",
      "\n",
      " in experimental design is it necessary to do randomization if yes why\n",
      "[(1, 1), (18, 1), (28, 1), (29, 1), (59, 1), (83, 1), (192, 1), (255, 1), (256, 1), (257, 1), (258, 1), (259, 1)]\n",
      "\n",
      " can you cite some examples where a false positive is important than a false negative\n",
      "[(0, 2), (1, 1), (27, 1), (48, 1), (53, 1), (219, 2), (260, 1), (261, 1), (262, 1), (263, 1), (264, 1), (265, 1), (266, 1)]\n",
      "\n",
      " can you cite some examples where a false negative important than a false positive\n",
      "[(0, 2), (27, 1), (48, 1), (53, 1), (219, 2), (260, 1), (261, 1), (262, 1), (263, 1), (264, 1), (265, 1), (266, 1)]\n",
      "\n",
      " can you cite some examples where both false positive and false negatives are equally important\n",
      "[(5, 1), (27, 1), (40, 1), (48, 1), (53, 1), (219, 2), (221, 1), (260, 1), (261, 1), (263, 1), (264, 1), (266, 1), (267, 1), (268, 1)]\n",
      "\n",
      " can you explain the difference between a test set and a validation set\n",
      "[(0, 2), (5, 1), (12, 1), (15, 1), (48, 1), (53, 1), (136, 1), (139, 1), (269, 2), (270, 1), (271, 1)]\n",
      "\n",
      " what makes a dataset gold standard\n",
      "[(0, 1), (4, 1), (272, 1), (273, 1), (274, 1), (275, 1)]\n",
      "\n",
      " what do you understand by statistical power of sensitivity and how do you calculate it\n",
      "[(4, 1), (5, 1), (14, 1), (18, 2), (19, 1), (43, 1), (47, 1), (48, 2), (52, 1), (148, 1), (192, 1), (276, 1), (277, 1)]\n",
      "\n",
      " what is the importance of having a selection bias\n",
      "[(0, 1), (1, 1), (4, 1), (14, 1), (15, 1), (106, 1), (278, 1), (279, 1), (280, 1)]\n",
      "\n",
      " give some situations where you will use an svm over a random forest machine learning algorithm and viceversa\n",
      "[(0, 1), (5, 1), (22, 1), (35, 1), (36, 1), (48, 1), (96, 1), (97, 1), (114, 1), (115, 1), (173, 1), (209, 1), (264, 1), (266, 1), (281, 1), (282, 1), (283, 1), (284, 1)]\n",
      "\n",
      " what do you understand by feature vectors\n",
      "[(4, 1), (18, 1), (43, 1), (47, 1), (48, 1), (63, 1), (64, 1)]\n",
      "\n",
      " how do data management procedures like missing data handling make selection bias worse\n",
      "[(17, 2), (18, 1), (19, 1), (106, 1), (153, 1), (176, 1), (280, 1), (285, 1), (286, 1), (287, 1), (288, 1), (289, 1)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sent, embedding in zip(sentences, bow_corpus):\n",
    "    print(sent)\n",
    "    print(embedding)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to get the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = None\n",
    "\n",
    "def PrintAnswer(question_embedding, sentence_embedding, df, sentences):\n",
    "    max_sim = 1\n",
    "    ind_sim = -1 \n",
    "    for index, faq_embedding in enumerate(sentence_embedding):\n",
    "        sim = cosine_similarity(faq_embedding, question_embedding)[0][0]\n",
    "        print(index, sim, sentences[index])\n",
    "        if sim > max_sim:\n",
    "            max_sim = sim\n",
    "            ind_sim = index\n",
    "    print('\\n')\n",
    "    print('Question: ', question)\n",
    "    print('\\n')\n",
    "    print('Retrieved ', df.iloc[ind_sim, 0])\n",
    "    print(df.iloc[ind_sim, 1])            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the function with a question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"why data cleansing is important in data analysis\"\n",
    "question = clean_sentences(question, stop_words = True)\n",
    "question_embedding = dictionary.doc2bow(question.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data cleansing important data analysis \n",
      " [(17, 2), (25, 1), (26, 1), (27, 1)]\n"
     ]
    }
   ],
   "source": [
    "print(question, '\\n', question_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.1168412475673972  what is a recommender system\n",
      "1 0.9967787445474133  compare sas r and python programming\n",
      "2 0.9999791889951382  explain the various benefits of r language\n",
      "3 0.9982979513032774  how do data scientists use statistics\n",
      "4 0.7848827655334262  what is logistic regression\n",
      "5 0.7848827655334262  why data cleansing is important in data analysis\n",
      "6 0.9967787445474133  describe univariate bivariate and multivariate analysis\n",
      "7 0.7848827655334262  how machine learning is deployed in real world scenarios\n",
      "8 0.1168412475673972  what are the various aspects of a machine learning process\n",
      "9 0.991835775307426  what do you understand by the term normal distribution\n",
      "10 0.7848827655334262  what is linear regression\n",
      "11 0.7848827655334262  what is interpolation and extrapolation\n",
      "12 0.7848827655334262  what is power analysis\n",
      "13 0.7848827655334262  what is kmeans  how can you select k for kmeans\n",
      "14 0.7848827655334262  how is data modeling different from database design\n",
      "15 0.991835775307426  what are feature vectors\n",
      "16 0.1168412475673972  explain the steps in making a decision tree\n",
      "17 0.7848827655334262  what is root cause analysis\n",
      "18 0.999423159507245  explain crossvalidation\n",
      "19 0.7848827655334262  what is collaborative filtering\n",
      "20 0.9981026894736714  do gradient descent methods at all times converge to similar point\n",
      "21 0.7848827655334262  what is the goal of ab testing\n",
      "22 0.991835775307426  what are the drawbacks of linear model\n",
      "23 0.7848827655334262  what is the law of large numbers\n",
      "24 0.991835775307426  what are confounding variables\n",
      "25 0.999423159507245  explain star schema\n",
      "26 0.9979189403219313  how regularly an algorithm must be update\n",
      "27 0.991835775307426  what are eigenvalue and eigenvector\n",
      "28 0.7848827655334262  why is resampling done\n",
      "29 0.999423159507245  explain selective bias\n",
      "30 0.991835775307426  what are the types of biases that can occur during sampling\n",
      "31 0.1168412475673972  how to work towards a random forest\n",
      "32 0.9999737460454936  python or r  which one would you prefer for text analytics\n",
      "33 0.7848827655334262  what is logistic regression or state an example when you have used logistic regression recently\n",
      "34 0.9405538996916954  what are recommender systems\n",
      "35 0.1168412475673972  why data cleaning plays a vital role in analysis\n",
      "36 0.9967787445474133  differentiate between univariate bivariate and multivariate analysis\n",
      "37 0.991835775307426  what do you understand by the term normal distribution\n",
      "38 0.7848827655334262  what is linear regression\n",
      "39 0.7848827655334262  what is interpolation and extrapolation\n",
      "40 0.7848827655334262  what is power analysis\n",
      "41 0.7848827655334262  what is collaborative filtering\n",
      "42 0.7848827655334262  what is the difference between cluster and systematic sampling\n",
      "43 0.9967787445474133  are expected value and mean value different\n",
      "44 0.991835775307426  what does pvalue signify about the statistical data\n",
      "45 0.9981026894736714  do gradient descent methods always converge to same point\n",
      "46 0.991835775307426  what are categorical variables\n",
      "47 0.9982979513032774  how you can make data normal using boxcox transformation\n",
      "48 0.7848827655334262  what is the difference between supervised learning an unsupervised learning\n",
      "49 0.999423159507245  explain the use of combinatorics in data science\n",
      "50 0.7848827655334262  what is vectorization\n",
      "51 0.7848827655334262  what is the goal of ab testing\n",
      "52 0.7848827655334262  what is an eigenvalue and eigenvector\n",
      "53 0.7848827655334262  what is gradient descent\n",
      "54 0.9979189403219313  how can outlier values be treated\n",
      "55 0.1168412475673972  how can you assess a good logistic model\n",
      "56 0.991835775307426  what are various steps involved in an analytics project\n",
      "57 0.1168412475673972  how can you iterate over a list and also retrieve element indices at the same time\n",
      "58 0.9981026894736714  during analysis how do you treat missing values\n",
      "59 0.999423159507245  explain about the box cox transformation in regression models\n",
      "60 0.9974316971602529  can you use machine learning for time series analysis\n",
      "61 0.7848827655334262  what is the difference between bayesian estimate and maximum likelihood estimation mle\n",
      "62 0.7848827655334262  what is regularization and what kind of problems does regularization solve\n",
      "63 0.7848827655334262  what is multicollinearity and how you can overcome it\n",
      "64 0.7848827655334262  what is the curse of dimensionality\n",
      "65 0.9987230966862342  how do you decide whether your linear regression model fits the data\n",
      "66 0.7848827655334262  what is the difference between mean squared error and mean absolute error\n",
      "67 0.7848827655334262  what is machine learning\n",
      "68 0.9967787445474133  how are confidence intervals constructed and how will you interpret them\n",
      "69 0.9979189403219313  how can you overcome overfitting\n",
      "70 0.9967787445474133  differentiate between wide and tall data formats\n",
      "71 0.1168412475673972  how will you define the number of clusters in a clustering algorithm\n",
      "72 0.7848827655334262  is it better to have too many false negatives or too many false positives\n",
      "73 0.991835775307426  what do you understand by fuzzy merging which language will you use to handle it\n",
      "74 0.7848827655334262  what is the difference between skewed and uniform distribution\n",
      "75 0.1168412475673972  you created a predictive model of a quantitative outcome variable using multiple regressions what are the steps you would follow to validate the model\n",
      "76 0.991835775307426  what do you understand by hypothesis in the content of machine learning\n",
      "77 0.991835775307426  what do you understand by recall and precision\n",
      "78 0.9987230966862342  how will you find the right k value for kmeans\n",
      "79 0.996587287525055  why l regularizations causes parameter sparsity whereas l regularization does not\n",
      "80 0.998951304730089  how can you deal with different types of seasonality in time series modelling\n",
      "81 0.7848827655334262  in experimental design is it necessary to do randomization if yes why\n",
      "82 0.1168412475673972  can you cite some examples where a false positive is important than a false negative\n",
      "83 0.1168412475673972  can you cite some examples where a false negative important than a false positive\n",
      "84 0.9967787445474133  can you cite some examples where both false positive and false negatives are equally important\n",
      "85 0.1168412475673972  can you explain the difference between a test set and a validation set\n",
      "86 0.1168412475673972  what makes a dataset gold standard\n",
      "87 0.991835775307426  what do you understand by statistical power of sensitivity and how do you calculate it\n",
      "88 0.1168412475673972  what is the importance of having a selection bias\n",
      "89 0.1168412475673972  give some situations where you will use an svm over a random forest machine learning algorithm and viceversa\n",
      "90 0.991835775307426  what do you understand by feature vectors\n",
      "91 0.9999999999999999  how do data management procedures like missing data handling make selection bias worse\n",
      "\n",
      "\n",
      "Question:  data cleansing important data analysis\n",
      "\n",
      "\n",
      "Retrieved  100. How do data management procedures like missing data handling make selection bias worse\n",
      " Missing value treatment is one of the primary tasks which a data scientist is supposed to do before starting data analysis. There are multiple methods for missing value treatment. If not done properly, it could potentially result into selection bias. Let see few missing value treatment examples and their impact on selection- Complete Case Treatment: Complete case treatment is when you remove entire row in data even if one value is missing. You could achieve a selection bias if your values are not missing at random and they have some pattern. Assume you are conducting a survey and few people didn’t specify their gender. Would you remove all those people, Can’t it tell a different story. Available case analysis: Let say you are trying to calculate correlation matrix for data so you might remove the missing values from variables which are needed for that particular correlation coefficient. In this case your values will not be fully correct as they are coming from population sets. Mean Substitution: In this method missing values are replaced with mean of other available values.  This might make your distribution biased e.g., standard deviation, correlation and regression are mostly dependent on the mean value of variables. Hence, various data management procedures might include selection bias in your data if not chosen correctly.\n"
     ]
    }
   ],
   "source": [
    "PrintAnswer(question_embedding, bow_corpus, data, sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"BOW\" method is not working properly, hence another method can be used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 'Glove' and 'Word2Vec' embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading Glove and Word2Vec models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Glove Model\n"
     ]
    }
   ],
   "source": [
    "glove_model = None\n",
    "try:\n",
    "    glove_model = gensim.model.KeyedVectors.load('./glovemodel.mod')\n",
    "    print(\"Loaded Glove Model\")\n",
    "except:\n",
    "    glove_model = api.load(\"glove-twitter-25\")\n",
    "    glove_model.save(\"./glovemodel.mod\")\n",
    "    print(\"Saved Glove Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n",
      "Saved v2w Model\n"
     ]
    }
   ],
   "source": [
    "v2w_model = None\n",
    "try:\n",
    "    v2w_model = gensim.model.KeyedVectors.load('./w2vecmodel.mod')\n",
    "    print(\"Loaded v2w Model\")\n",
    "except:\n",
    "    v2w_model = api.load(\"word2vec-google-news-300\")\n",
    "    v2w_model.save('./w2vecmodel.mod')\n",
    "    print(\"Saved v2w Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vec_embedding_size = len(v2w_model['computer'])\n",
    "glove_embedding_size = len(glove_model['computer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function to take question and retrieve answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordvec(word, model):\n",
    "    samp = model['computer']\n",
    "    vec = [0]*len(samp)\n",
    "    try:\n",
    "        vec = model[word]\n",
    "    except:\n",
    "        vec = [0]*len(samp)\n",
    "    return (vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPhraseEmbedding(phrase, embeddingmodel):\n",
    "    samp = getWordvec('computer', embeddingmodel)\n",
    "    vec = np.array([0]*len(samp))\n",
    "    den = 0\n",
    "    for word in phrase.split():\n",
    "        den = den+1\n",
    "        vec = vec+np.array(getWordvec(word, embeddingmodel))\n",
    "    return vec.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving relevant question and answer with Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_embeddings = []\n",
    "for sent in cleaned_sentences:\n",
    "    sent_embeddings.append(getPhraseEmbedding(sent, v2w_model))\n",
    "\n",
    "question_embedding = getPhraseEmbedding(question, v2w_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.2767839148134592  recommender\n",
      "1 0.18974432860954218  compare sas r python programming\n",
      "2 0.2709074773592258  explain benefits r language\n",
      "3 0.6974599068160648  data scientists use statistics\n",
      "4 0.31533645833365354  logistic regression\n",
      "5 1.0000000000000002  data cleansing important data analysis\n",
      "6 0.5232633654234448  univariate bivariate multivariate analysis\n",
      "7 0.30585343340666665  machine learning deployed real world scenarios\n",
      "8 0.33050077664146615  aspects machine learning process\n",
      "9 0.31022701003870323  understand term normal distribution\n",
      "10 0.2961850807011226  linear regression\n",
      "11 0.2915158876889972  interpolation extrapolation\n",
      "12 0.5848069589504791  power analysis\n",
      "13 0.09370918701021415  kmeans select k kmeans\n",
      "14 0.6759878173315197  data modeling different database design\n",
      "15 0.22180762994886202  feature vectors\n",
      "16 0.19314132010618892  explain steps making decision tree\n",
      "17 0.47140846268992737  root cause analysis\n",
      "18 0.1082753264234256  explain crossvalidation\n",
      "19 0.3686834580123191  collaborative filtering\n",
      "20 0.26182653800149347  gradient descent methods times converge similar point\n",
      "21 0.2649539386236969  goal ab testing\n",
      "22 0.2602750959592249  drawbacks linear model\n",
      "23 0.2712319083525193  law large numbers\n",
      "24 0.33922626548462687  confounding variables\n",
      "25 0.22124865422862083  explain star schema\n",
      "26 0.40137003761394463  regularly algorithm update\n",
      "27 0.3679080559894886  eigenvalue eigenvector\n",
      "28 0.4045506350196371  resampling\n",
      "29 0.27854052326849654  explain selective bias\n",
      "30 0.42800004360083865  types biases occur sampling\n",
      "31 0.21983075174141695  work random forest\n",
      "32 0.3808839097468449  python r prefer text analytics\n",
      "33 0.34192944470450953  logistic regression state example logistic regression recently\n",
      "34 0.4118045625250227  recommender systems\n",
      "35 0.7195957391035588  data cleaning plays vital role analysis\n",
      "36 0.5185068708215509  differentiate univariate bivariate multivariate analysis\n",
      "37 0.31022701003870323  understand term normal distribution\n",
      "38 0.2961850807011226  linear regression\n",
      "39 0.2915158876889972  interpolation extrapolation\n",
      "40 0.5848069589504791  power analysis\n",
      "41 0.3686834580123191  collaborative filtering\n",
      "42 0.41189336422508155  difference cluster systematic sampling\n",
      "43 0.29970299765798386  expected value mean value different\n",
      "44 0.6812472909973145  pvalue signify statistical data\n",
      "45 0.2577911664146087  gradient descent methods converge point\n",
      "46 0.3230337317185767  categorical variables\n",
      "47 0.6831706254843483  data normal boxcox transformation\n",
      "48 0.17289407019633482  difference supervised learning unsupervised learning\n",
      "49 0.5783844590576652  explain use combinatorics data science\n",
      "50 0.3081928014691618  vectorization\n",
      "51 0.2649539386236969  goal ab testing\n",
      "52 0.3679080559894886  eigenvalue eigenvector\n",
      "53 0.1617002040150013  gradient descent\n",
      "54 0.2708214304457246  outlier values treated\n",
      "55 0.3966083219702408  assess good logistic model\n",
      "56 0.452200288573685  steps involved analytics project\n",
      "57 0.45466064257457717  iterate list retrieve element indices time\n",
      "58 0.49680904716324176  analysis treat missing values\n",
      "59 0.27905314255903496  explain box cox transformation regression models\n",
      "60 0.5014995208819436  use machine learning time series analysis\n",
      "61 0.35455162082537256  difference bayesian estimate maximum likelihood estimation mle\n",
      "62 0.24499282264609015  regularization kind problems regularization solve\n",
      "63 0.05780672999244706  multicollinearity overcome\n",
      "64 0.10425656088215104  curse dimensionality\n",
      "65 0.5433253311056935  decide linear regression model fits data\n",
      "66 0.20650584707869474  difference mean squared error mean absolute error\n",
      "67 0.21121393361713683  machine learning\n",
      "68 0.3282072620963264  confidence intervals constructed interpret\n",
      "69 0.05780672999244706  overcome overfitting\n",
      "70 0.4297757112093471  differentiate wide tall data formats\n",
      "71 0.4203870636580367  define number clusters clustering algorithm\n",
      "72 0.2291617969077316  better false negatives false positives\n",
      "73 0.2750130488845946  understand fuzzy merging language use handle\n",
      "74 0.27163680294819814  difference skewed uniform distribution\n",
      "75 0.4994428097077316  created predictive model quantitative outcome variable multiple regressions steps follow validate model\n",
      "76 0.39057752397303874  understand hypothesis content machine learning\n",
      "77 0.19977564600053432  understand recall precision\n",
      "78 0.1931626490591607  right k value kmeans\n",
      "79 0.30335823623601577  l regularizations causes parameter sparsity l regularization\n",
      "80 0.28815048822707306  deal different types seasonality time series modelling\n",
      "81 0.358122896778332  experimental design necessary randomization yes\n",
      "82 0.3302351275846355  cite examples false positive important false negative\n",
      "83 0.3302351275846355  cite examples false negative important false positive\n",
      "84 0.3357651157359075  cite examples false positive false negatives equally important\n",
      "85 0.34148479768238144  explain difference test set validation set\n",
      "86 0.481153266644476  makes dataset gold standard\n",
      "87 0.47319629043884776  understand statistical power sensitivity calculate\n",
      "88 0.2918816414781103  importance having selection bias\n",
      "89 0.3440310128690154  situations use svm random forest machine learning algorithm viceversa\n",
      "90 0.29220935945163096  understand feature vectors\n",
      "91 0.6747111704721005  data management procedures like missing data handling selection bias worse\n",
      "\n",
      "\n",
      "Question:  data cleansing important data analysis\n",
      "\n",
      "\n",
      "Retrieved  6. Why Data Cleansing Is Important in Data Analysis\n",
      " With data coming in from multiple sources it is important to ensure that data is good enough for analysis. This is where data cleansing becomes extremely vital. Data cleansing extensively deals with the process of detecting and correcting of data records, ensuring that data is complete and accurate and the components of data that are irrelevant are deleted or modified as per the needs. This process can be deployed in concurrence with data wrangling or batch processing. Once the data is cleaned it confirms with the rules of the data sets in the system. Data cleansing is an essential part of the data science because the data can be prone to error due to human negligence, corruption during transmission or storage among other things. Data cleansing takes a huge chunk of time and effort of a Data Scientist because of the multiple sources from which data emanates and the speed at which it comes.\n"
     ]
    }
   ],
   "source": [
    "PrintAnswer(question_embedding, sent_embeddings, data, cleaned_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving relevant question and answer with Glove model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_embeddings = []\n",
    "for sent in cleaned_sentences:\n",
    "    sent_embeddings.append(getPhraseEmbedding(sent, glove_model))\n",
    "\n",
    "question_embedding = getPhraseEmbedding(question, glove_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0  recommender\n",
      "1 0.7906008816119312  compare sas r python programming\n",
      "2 0.8051940294381376  explain benefits r language\n",
      "3 0.9047266490771675  data scientists use statistics\n",
      "4 0.5503523856669136  logistic regression\n",
      "5 1.0000000000000004  data cleansing important data analysis\n",
      "6 0.8762196779298578  univariate bivariate multivariate analysis\n",
      "7 0.7728242751003606  machine learning deployed real world scenarios\n",
      "8 0.9050118345161404  aspects machine learning process\n",
      "9 0.851060469415186  understand term normal distribution\n",
      "10 0.5955369823620535  linear regression\n",
      "11 0.0  interpolation extrapolation\n",
      "12 0.927302544897734  power analysis\n",
      "13 0.6184479288055786  kmeans select k kmeans\n",
      "14 0.9096092613148183  data modeling different database design\n",
      "15 0.6481355473096122  feature vectors\n",
      "16 0.7731342956127127  explain steps making decision tree\n",
      "17 0.8291412880145512  root cause analysis\n",
      "18 0.6273381969826294  explain crossvalidation\n",
      "19 0.7053778512714018  collaborative filtering\n",
      "20 0.820619950598189  gradient descent methods times converge similar point\n",
      "21 0.7667565394842382  goal ab testing\n",
      "22 0.7318993673546904  drawbacks linear model\n",
      "23 0.8523998950278221  law large numbers\n",
      "24 0.3842751227692289  confounding variables\n",
      "25 0.713346248793815  explain star schema\n",
      "26 0.8370127883738643  regularly algorithm update\n",
      "27 0.0  eigenvalue eigenvector\n",
      "28 0.0  resampling\n",
      "29 0.6996830821869151  explain selective bias\n",
      "30 0.6445840181095107  types biases occur sampling\n",
      "31 0.6767703020664362  work random forest\n",
      "32 0.7692723503965171  python r prefer text analytics\n",
      "33 0.8054992192658367  logistic regression state example logistic regression recently\n",
      "34 0.8586807248872314  recommender systems\n",
      "35 0.931691750022912  data cleaning plays vital role analysis\n",
      "36 0.8412294909521845  differentiate univariate bivariate multivariate analysis\n",
      "37 0.851060469415186  understand term normal distribution\n",
      "38 0.5955369823620535  linear regression\n",
      "39 0.0  interpolation extrapolation\n",
      "40 0.927302544897734  power analysis\n",
      "41 0.7053778512714018  collaborative filtering\n",
      "42 0.7964682764455768  difference cluster systematic sampling\n",
      "43 0.8043120539324891  expected value mean value different\n",
      "44 0.8590588900314512  pvalue signify statistical data\n",
      "45 0.7875502939137684  gradient descent methods converge point\n",
      "46 0.45246717941167536  categorical variables\n",
      "47 0.9024091681432637  data normal boxcox transformation\n",
      "48 0.7979563378286941  difference supervised learning unsupervised learning\n",
      "49 0.8794238276084234  explain use combinatorics data science\n",
      "50 0.0  vectorization\n",
      "51 0.7667565394842382  goal ab testing\n",
      "52 0.0  eigenvalue eigenvector\n",
      "53 0.4895196249700271  gradient descent\n",
      "54 0.6503754140202069  outlier values treated\n",
      "55 0.7904782919330172  assess good logistic model\n",
      "56 0.8978535674561336  steps involved analytics project\n",
      "57 0.8089257612500034  iterate list retrieve element indices time\n",
      "58 0.8372396423721824  analysis treat missing values\n",
      "59 0.7702039203305993  explain box cox transformation regression models\n",
      "60 0.8724806366036932  use machine learning time series analysis\n",
      "61 0.7742116094063204  difference bayesian estimate maximum likelihood estimation mle\n",
      "62 0.7362045704818356  regularization kind problems regularization solve\n",
      "63 0.591126406675733  multicollinearity overcome\n",
      "64 0.4452627500956375  curse dimensionality\n",
      "65 0.89920605760731  decide linear regression model fits data\n",
      "66 0.6758705061122734  difference mean squared error mean absolute error\n",
      "67 0.8663141438656307  machine learning\n",
      "68 0.6931314649255325  confidence intervals constructed interpret\n",
      "69 0.591126406675733  overcome overfitting\n",
      "70 0.8419880376001859  differentiate wide tall data formats\n",
      "71 0.7622239666163144  define number clusters clustering algorithm\n",
      "72 0.7170514761171483  better false negatives false positives\n",
      "73 0.7394115974151224  understand fuzzy merging language use handle\n",
      "74 0.8410758413395856  difference skewed uniform distribution\n",
      "75 0.8905978202331637  created predictive model quantitative outcome variable multiple regressions steps follow validate model\n",
      "76 0.8629608713486522  understand hypothesis content machine learning\n",
      "77 0.8194707003486801  understand recall precision\n",
      "78 0.742620255404831  right k value kmeans\n",
      "79 0.7268495946789388  l regularizations causes parameter sparsity l regularization\n",
      "80 0.7840723288319567  deal different types seasonality time series modelling\n",
      "81 0.8620453948319134  experimental design necessary randomization yes\n",
      "82 0.7891167981425576  cite examples false positive important false negative\n",
      "83 0.7891167981425576  cite examples false negative important false positive\n",
      "84 0.7637693940191175  cite examples false positive false negatives equally important\n",
      "85 0.8636688745652448  explain difference test set validation set\n",
      "86 0.7971925149611637  makes dataset gold standard\n",
      "87 0.8508160620338232  understand statistical power sensitivity calculate\n",
      "88 0.8330582822775557  importance having selection bias\n",
      "89 0.8170207704012192  situations use svm random forest machine learning algorithm viceversa\n",
      "90 0.7545870499869756  understand feature vectors\n",
      "91 0.9069202132854047  data management procedures like missing data handling selection bias worse\n",
      "\n",
      "\n",
      "Question:  data cleansing important data analysis\n",
      "\n",
      "\n",
      "Retrieved  6. Why Data Cleansing Is Important in Data Analysis\n",
      " With data coming in from multiple sources it is important to ensure that data is good enough for analysis. This is where data cleansing becomes extremely vital. Data cleansing extensively deals with the process of detecting and correcting of data records, ensuring that data is complete and accurate and the components of data that are irrelevant are deleted or modified as per the needs. This process can be deployed in concurrence with data wrangling or batch processing. Once the data is cleaned it confirms with the rules of the data sets in the system. Data cleansing is an essential part of the data science because the data can be prone to error due to human negligence, corruption during transmission or storage among other things. Data cleansing takes a huge chunk of time and effort of a Data Scientist because of the multiple sources from which data emanates and the speed at which it comes.\n"
     ]
    }
   ],
   "source": [
    "PrintAnswer(question_embedding, sent_embeddings, data, cleaned_sentences)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
